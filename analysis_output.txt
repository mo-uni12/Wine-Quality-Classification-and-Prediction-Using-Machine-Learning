Wine Quality Classification and Prediction Using Machine Learning
======================================================================
Loading wine quality datasets...
Red wine samples: 1599
White wine samples: 4898
Total samples: 6497

==================================================
EXPLORATORY DATA ANALYSIS
==================================================

Dataset Info:
<class 'pandas.core.frame.DataFrame'>
RangeIndex: 6497 entries, 0 to 6496
Data columns (total 13 columns):
 #   Column                Non-Null Count  Dtype  
---  ------                --------------  -----  
 0   fixed acidity         6497 non-null   float64
 1   volatile acidity      6497 non-null   float64
 2   citric acid           6497 non-null   float64
 3   residual sugar        6497 non-null   float64
 4   chlorides             6497 non-null   float64
 5   free sulfur dioxide   6497 non-null   float64
 6   total sulfur dioxide  6497 non-null   float64
 7   density               6497 non-null   float64
 8   pH                    6497 non-null   float64
 9   sulphates             6497 non-null   float64
 10  alcohol               6497 non-null   float64
 11  quality               6497 non-null   int64  
 12  wine_type             6497 non-null   object 
dtypes: float64(11), int64(1), object(1)
memory usage: 660.0+ KB
None

Dataset Shape: (6497, 13)

Feature Names: ['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar', 'chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'density', 'pH', 'sulphates', 'alcohol', 'quality', 'wine_type']

Statistical Summary:
       fixed acidity  volatile acidity  ...      alcohol      quality
count    6497.000000       6497.000000  ...  6497.000000  6497.000000
mean        7.215307          0.339666  ...    10.491801     5.818378
std         1.296434          0.164636  ...     1.192712     0.873255
min         3.800000          0.080000  ...     8.000000     3.000000
25%         6.400000          0.230000  ...     9.500000     5.000000
50%         7.000000          0.290000  ...    10.300000     6.000000
75%         7.700000          0.400000  ...    11.300000     6.000000
max        15.900000          1.580000  ...    14.900000     9.000000

[8 rows x 12 columns]

Missing Values:
Series([], dtype: int64)

Quality Distribution:
quality
3      30
4     216
5    2138
6    2836
7    1079
8     193
9       5
Name: count, dtype: int64

Wine Type Distribution:
wine_type
white    4898
red      1599
Name: count, dtype: int64

Generating visualizations...
Visualizations saved successfully!

Preprocessing data...
Training set size: 5197
Test set size: 1300
Feature count: 11
Class distribution in training set:
Poor/Average (0): 4176
Good (1): 1021

Training machine learning models...

Training Random Forest...
Accuracy: 0.8823
CV Score: 0.8790 (+/- 0.0111)

Training Gradient Boosting...
Accuracy: 0.8415
CV Score: 0.8386 (+/- 0.0070)

Training SVM...
Accuracy: 0.8308
CV Score: 0.8353 (+/- 0.0096)

Training Logistic Regression...
Accuracy: 0.8215
CV Score: 0.8178 (+/- 0.0090)

Training K-Nearest Neighbors...
Accuracy: 0.8315
CV Score: 0.8457 (+/- 0.0076)

Training Naive Bayes...
Accuracy: 0.7508
CV Score: 0.7441 (+/- 0.0276)

==================================================
MODEL EVALUATION RESULTS
==================================================

Model Performance Comparison:
              Model  Accuracy  CV Mean   CV Std
      Random Forest  0.882308 0.878970 0.005565
  Gradient Boosting  0.841538 0.838562 0.003498
K-Nearest Neighbors  0.831538 0.845681 0.003825
                SVM  0.830769 0.835291 0.004809
Logistic Regression  0.821538 0.817778 0.004515
        Naive Bayes  0.750769 0.744087 0.013806

Detailed Classification Reports:

Random Forest:
              precision    recall  f1-score   support

           0       0.90      0.96      0.93      1044
           1       0.78      0.55      0.65       256

    accuracy                           0.88      1300
   macro avg       0.84      0.76      0.79      1300
weighted avg       0.88      0.88      0.87      1300


Gradient Boosting:
              precision    recall  f1-score   support

           0       0.87      0.95      0.91      1044
           1       0.66      0.40      0.50       256

    accuracy                           0.84      1300
   macro avg       0.76      0.68      0.70      1300
weighted avg       0.83      0.84      0.83      1300


SVM:
              precision    recall  f1-score   support

           0       0.85      0.97      0.90      1044
           1       0.67      0.28      0.39       256

    accuracy                           0.83      1300
   macro avg       0.76      0.62      0.65      1300
weighted avg       0.81      0.83      0.80      1300


Logistic Regression:
              precision    recall  f1-score   support

           0       0.84      0.96      0.90      1044
           1       0.61      0.26      0.37       256

    accuracy                           0.82      1300
   macro avg       0.73      0.61      0.63      1300
weighted avg       0.80      0.82      0.79      1300


K-Nearest Neighbors:
              precision    recall  f1-score   support

           0       0.88      0.92      0.90      1044
           1       0.59      0.48      0.53       256

    accuracy                           0.83      1300
   macro avg       0.73      0.70      0.71      1300
weighted avg       0.82      0.83      0.82      1300


Naive Bayes:
              precision    recall  f1-score   support

           0       0.89      0.78      0.83      1044
           1       0.41      0.62      0.50       256

    accuracy                           0.75      1300
   macro avg       0.65      0.70      0.66      1300
weighted avg       0.80      0.75      0.77      1300


Generating evaluation plots...
Evaluation plots saved successfully!

Performing hyperparameter tuning...
